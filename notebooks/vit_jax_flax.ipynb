{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zlFHtNbVSdy"
      },
      "outputs": [],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hlqlG3OU3Zw"
      },
      "outputs": [],
      "source": [
        "!ls -l /usr/lib/x86_64-linux-gnu/libcudnn.so*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-zKamopbA21"
      },
      "outputs": [],
      "source": [
        "!cat /usr/include/cudnn_version.h | grep MAJOR -A 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bahRRh3bVavr"
      },
      "outputs": [],
      "source": [
        "!echo $LD_LIBRARY_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ktBNz7HKxUS"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "# Installs the wheel compatible with CUDA 11 and cuDNN 8.2 or newer.\n",
        "# Note: wheels only available on linux.\n",
        "#!pip install --upgrade \"jax[cuda]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ot7UVoRW9HP6"
      },
      "outputs": [],
      "source": [
        "!pip install flax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXtMFKHOYkvz"
      },
      "outputs": [],
      "source": [
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGT0h4ZRK3W2"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.dlpack\n",
        "from jax import grad, jit, vmap, random\n",
        "from jax import random\n",
        "from jax.example_libraries import stax, optimizers\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "import numpy.random as npr\n",
        "import math\n",
        "\n",
        "from typing import Optional\n",
        "\n",
        "import optax\n",
        "from flax.training import train_state, checkpoints\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNmAau7872pm"
      },
      "source": [
        "## ViT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIYx16Xv896d"
      },
      "outputs": [],
      "source": [
        "from flax import linen as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmUlFQSBL1Gw"
      },
      "outputs": [],
      "source": [
        "class Patches(nn.Module):\n",
        "  patch_size: int\n",
        "  embed_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.conv = nn.Conv(\n",
        "        features=self.embed_dim,\n",
        "        kernel_size=(self.patch_size, self.patch_size),\n",
        "        strides=(self.patch_size, self.patch_size),\n",
        "        padding='VALID'\n",
        "    )\n",
        "\n",
        "  def __call__(self, images):\n",
        "    patches = self.conv(images)\n",
        "    b, h, w, c = patches.shape\n",
        "    patches = jnp.reshape(patches, (b, h*w, c))\n",
        "    return patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkA3rripeet2"
      },
      "outputs": [],
      "source": [
        "class PatchEncoder(nn.Module):\n",
        "  hidden_dim: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    assert x.ndim == 3\n",
        "    n, seq_len, _ = x.shape\n",
        "    # Hidden dim\n",
        "    x = nn.Dense(self.hidden_dim)(x)\n",
        "    # Add cls token\n",
        "    cls = self.param('cls_token', nn.initializers.zeros, (1, 1, self.hidden_dim))\n",
        "    cls = jnp.tile(cls, (n, 1, 1))\n",
        "    x = jnp.concatenate([cls, x], axis=1)\n",
        "    # Add position embedding\n",
        "    pos_embed = self.param(\n",
        "        'position_embedding', \n",
        "        nn.initializers.normal(stddev=0.02), # From BERT\n",
        "        (1, seq_len + 1, self.hidden_dim)\n",
        "    )\n",
        "    return x + pos_embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03qMlD1p72M8"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "  hidden_dim: int\n",
        "  n_heads: int\n",
        "  drop_p: float\n",
        "\n",
        "  def setup(self):\n",
        "    self.q_net = nn.Dense(self.hidden_dim)\n",
        "    self.k_net = nn.Dense(self.hidden_dim)\n",
        "    self.v_net = nn.Dense(self.hidden_dim)\n",
        "\n",
        "    self.proj_net = nn.Dense(self.hidden_dim)\n",
        "\n",
        "    self.att_drop = nn.Dropout(self.drop_p)\n",
        "    self.proj_drop = nn.Dropout(self.drop_p)\n",
        "\n",
        "  def __call__(self, x, train=True):\n",
        "    B, T, C = x.shape # batch_size, seq_length, hidden_dim\n",
        "    N, D = self.n_heads, C // self.n_heads # num_heads, head_dim\n",
        "    q = self.q_net(x).reshape(B, T, N, D).transpose(0, 2, 1, 3) # (B, N, T, D)\n",
        "    k = self.k_net(x).reshape(B, T, N, D).transpose(0, 2, 1, 3)\n",
        "    v = self.v_net(x).reshape(B, T, N, D).transpose(0, 2, 1, 3)\n",
        "\n",
        "    # weights (B, N, T, T)\n",
        "    weights = jnp.matmul(q, jnp.swapaxes(k, -2, -1)) / math.sqrt(D)\n",
        "    normalized_weights = nn.softmax(weights, axis=-1)\n",
        "\n",
        "    # attention (B, N, T, D)\n",
        "    attention = jnp.matmul(normalized_weights, v)\n",
        "    attention = self.att_drop(attention, deterministic=not train)\n",
        "\n",
        "    # gather heads\n",
        "    attention = attention.transpose(0, 2, 1, 3).reshape(B, T, N*D)\n",
        "\n",
        "    # project\n",
        "    out = self.proj_drop(self.proj_net(attention), deterministic=not train)\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ83UwjTQm9r"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "  mlp_dim: int\n",
        "  drop_p: float\n",
        "  out_dim: Optional[int] = None\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs, train=True):\n",
        "    actual_out_dim = inputs.shape[-1] if self.out_dim is None else self.out_dim\n",
        "    x = nn.Dense(features=self.mlp_dim)(inputs)\n",
        "    x = nn.gelu(x)\n",
        "    x = nn.Dropout(rate=self.drop_p, deterministic=not train)(x)\n",
        "    x = nn.Dense(features=actual_out_dim)(x)\n",
        "    x = nn.Dropout(rate=self.drop_p, deterministic=not train)(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oi4p-2JcHej8"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  embed_dim: int\n",
        "  hidden_dim: int\n",
        "  n_heads: int\n",
        "  drop_p: float\n",
        "  mlp_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.mha = MultiHeadSelfAttention(self.hidden_dim, self.n_heads, self.drop_p)\n",
        "    self.mlp = MLP(self.mlp_dim, self.drop_p)\n",
        "    self.layer_norm = nn.LayerNorm(epsilon=1e-6)\n",
        "  \n",
        "  def __call__(self, inputs, train=True):\n",
        "    # Attention Block\n",
        "    x = self.layer_norm(inputs)\n",
        "    x = self.mha(x, train)\n",
        "    x = inputs + x\n",
        "    # MLP block\n",
        "    y = self.layer_norm(x)\n",
        "    y = self.mlp(y, train)\n",
        "\n",
        "    return x + y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eq_xl6tFBrTj"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "  patch_size: int\n",
        "  embed_dim: int\n",
        "  hidden_dim: int\n",
        "  n_heads: int\n",
        "  drop_p: float\n",
        "  num_layers: int\n",
        "  mlp_dim: int\n",
        "  num_classes: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.patch_extracter = Patches(self.patch_size, self.embed_dim)\n",
        "    self.patch_encoder = PatchEncoder(self.hidden_dim)\n",
        "    self.dropout = nn.Dropout(self.drop_p)\n",
        "    self.transformer_blocks = [\n",
        "      TransformerEncoder(self.embed_dim, self.hidden_dim, self.n_heads, self.drop_p, self.mlp_dim)\n",
        "      for _ in range(self.num_layers)]\n",
        "    self.cls_head = nn.Dense(features=self.num_classes)\n",
        "\n",
        "  def __call__(self, x, train=True):\n",
        "    x = self.patch_extracter(x)\n",
        "    x = self.patch_encoder(x)\n",
        "    x = self.dropout(x, deterministic=not train)\n",
        "    for block in self.transformer_blocks:\n",
        "      x = block(x, train)\n",
        "    # MLP head\n",
        "    x = x[:, 0] # [CLS] token\n",
        "    x = self.cls_head(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cyq9sVnClSdK"
      },
      "source": [
        "## Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37TDI0wIlVVd"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 32\n",
        "BATCH_SIZE = 128\n",
        "DATA_MEANS = np.array([0.49139968, 0.48215841, 0.44653091])\n",
        "DATA_STD = np.array([0.24703223, 0.24348513, 0.26158784])\n",
        "CROP_SCALES = (0.8, 1.0)\n",
        "CROP_RATIO = (0.9, 1.1)\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ytr_d-335mt"
      },
      "source": [
        "## Dataset preparation(torchvision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkIV9zHp4C_f"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3u7GUZ9b4SWU"
      },
      "outputs": [],
      "source": [
        "def image_to_numpy(img):\n",
        "  img = np.array(img, dtype=np.float32)\n",
        "  img = (img / 255. - DATA_MEANS) / DATA_STD\n",
        "  return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0s3OIso4xHy"
      },
      "outputs": [],
      "source": [
        "# We need to stack the batch elements\n",
        "def numpy_collate(batch):\n",
        "  if isinstance(batch[0], np.ndarray):\n",
        "    return np.stack(batch)\n",
        "  elif isinstance(batch[0], (tuple, list)):\n",
        "    transposed = zip(*batch)\n",
        "    return [numpy_collate(samples) for samples in transposed]\n",
        "  else:\n",
        "    return np.array(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lynE6uoGLEeq"
      },
      "outputs": [],
      "source": [
        "test_transform = image_to_numpy\n",
        "# For training, we add some augmentations. Neworks are too powerful and would overfit.\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomResizedCrop((IMAGE_SIZE, IMAGE_SIZE), scale=CROP_SCALES, ratio=CROP_RATIO),\n",
        "    image_to_numpy\n",
        "])\n",
        "\n",
        "# Validation set should not use the augmentation.\n",
        "train_dataset = CIFAR10('data', train=True, transform=train_transform, download=True)\n",
        "val_dataset = CIFAR10('data', train=True, transform=test_transform, download=True)\n",
        "train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000], generator=torch.Generator().manual_seed(SEED))\n",
        "_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000], generator=torch.Generator().manual_seed(SEED))\n",
        "test_set = CIFAR10('data', train=False, transform=test_transform, download=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=2, persistent_workers=True, collate_fn=numpy_collate,\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=2, persistent_workers=True, collate_fn=numpy_collate,\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=2, persistent_workers=True, collate_fn=numpy_collate,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tmgh8MFknu4M"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_loader))\n",
        "print(f'image: {batch[0].shape}, label: {batch[1].shape}')\n",
        "print(f'type batch[0]: {type(batch[0])}, batch[1]: {type(batch[1])}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIqoANBI71qj"
      },
      "outputs": [],
      "source": [
        "# Visualize some examples\n",
        "def numpy_to_torch(array):\n",
        "  array = jax.device_get(array)\n",
        "  tensor = torch.from_numpy(array)\n",
        "  tensor = tensor.permute(0, 3, 1, 2)\n",
        "  return tensor\n",
        "\n",
        "NUM_IMAGES = 8\n",
        "CIFAR_images = np.stack([val_set[idx][0] for idx in range(NUM_IMAGES)], axis=0)\n",
        "img_grid = torchvision.utils.make_grid(numpy_to_torch(CIFAR_images), nrow=4, normalize=True, pad_value=0.9)\n",
        "img_grid = img_grid.permute(1, 2, 0)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.title(\"Image examples of the CIFAR10 dataset\")\n",
        "plt.imshow(img_grid)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCcAE57QLZss"
      },
      "source": [
        "## Initialize model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCz8JPL2NY9O"
      },
      "outputs": [],
      "source": [
        "def initialize_model(\n",
        "    seed=42,\n",
        "    patch_size=4, embed_dim=64, hidden_dim=192,\n",
        "    n_heads=3, drop_p=0.1, num_layers=12, mlp_dim=768, num_classes=10\n",
        "):\n",
        "  main_rng = jax.random.PRNGKey(seed)\n",
        "  x = jnp.ones(shape=(5, 32, 32, 3))\n",
        "  # ViT\n",
        "  model = ViT(\n",
        "      patch_size=patch_size,\n",
        "      embed_dim=embed_dim,\n",
        "      hidden_dim=hidden_dim,\n",
        "      n_heads=n_heads,\n",
        "      drop_p=drop_p,\n",
        "      num_layers=num_layers,\n",
        "      mlp_dim=mlp_dim,\n",
        "      num_classes=num_classes\n",
        "  )\n",
        "  main_rng, init_rng, drop_rng = random.split(main_rng, 3)\n",
        "  params = model.init({'params': init_rng, 'dropout': drop_rng}, x, train=True)['params']\n",
        "  return model, params, main_rng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUG69x8ganiH"
      },
      "outputs": [],
      "source": [
        "vit_model, vit_params, vit_rng = initialize_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaTTZRuCcDyc"
      },
      "outputs": [],
      "source": [
        "jax.tree_map(lambda x: x.shape, vit_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYu8XbQbfDx_"
      },
      "source": [
        "## Define loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-geg2pmOoOwX"
      },
      "outputs": [],
      "source": [
        "def calculate_loss(params, state, rng, batch, train):\n",
        "  imgs, labels = batch\n",
        "  rng, drop_rng = random.split(rng)\n",
        "  logits = state.apply_fn({'params': params}, imgs, train=train, rngs={'dropout': drop_rng})\n",
        "  loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=labels).mean()\n",
        "  acc = (logits.argmax(axis=-1) == labels).mean()\n",
        "  return loss, (acc, rng)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtEHCv7Nr23i"
      },
      "source": [
        "## Train step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLvmNll-pLzA"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def train_step(state, rng, batch):\n",
        "  loss_fn = lambda params: calculate_loss(params, state, rng, batch, train=True)\n",
        "  # Get loss, gradients for loss, and other outputs of loss function\n",
        "  (loss, (acc, rng)), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
        "  # Update parameters and batch statistics\n",
        "  state = state.apply_gradients(grads=grads)\n",
        "  return state, rng, loss, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17PIif-er6RP"
      },
      "source": [
        "## Evaluate step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyXw0Q1d89HR"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def eval_step(state, rng, batch):\n",
        "  _, (acc, rng) = calculate_loss(state.params, state, rng, batch, train=False)\n",
        "  return rng, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHXSxZ2o6_a_"
      },
      "outputs": [],
      "source": [
        "logger = SummaryWriter(log_dir='vit_jax_logs')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O408vD15sJl9"
      },
      "source": [
        "## Train function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Wj9i4EU2LL9"
      },
      "outputs": [],
      "source": [
        "def train_epoch(train_loader, epoch_idx, state, rng):\n",
        "  metrics = defaultdict(list)\n",
        "  for batch in tqdm(train_loader, desc='Training', leave=False):\n",
        "    state, rng, loss, acc = train_step(state, rng, batch)\n",
        "    metrics['loss'].append(loss)\n",
        "    metrics['acc'].append(acc)\n",
        "  for key in metrics.keys():\n",
        "    arg_val = np.stack(jax.device_get(metrics[key])).mean()\n",
        "    logger.add_scalar('train/' + key, arg_val, global_step=epoch_idx)\n",
        "    print(f'[epoch {epoch_idx}] {key}: {arg_val}')\n",
        "  return state, rng"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2NHLygSsK3v"
      },
      "source": [
        "## Evaluate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaOva65J8Ay2"
      },
      "outputs": [],
      "source": [
        "def eval_model(data_loader, state, rng):\n",
        "  # Test model on all images of a data loader and return avg loss\n",
        "  correct_class, count = 0, 0\n",
        "  for batch in data_loader:\n",
        "    rng, acc = eval_step(state, rng, batch)\n",
        "    correct_class += acc * batch[0].shape[0]\n",
        "    count += batch[0].shape[0]\n",
        "  eval_acc = (correct_class / count).item()\n",
        "  return eval_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovbKehyOsU8Q"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yLMra-C3-xC"
      },
      "outputs": [],
      "source": [
        "def train_model(train_loader, val_loader, state, rng, num_epochs=100):\n",
        "  best_eval = 0.0\n",
        "  for epoch_idx in tqdm(range(1, num_epochs + 1)):\n",
        "    state, rng = train_epoch(train_loader, epoch_idx, state, rng)\n",
        "    if epoch_idx % 1 == 0:\n",
        "      eval_acc = eval_model(val_loader, state, rng)\n",
        "      logger.add_scalar('val/acc', eval_acc, global_step=epoch_idx)\n",
        "      if eval_acc >= best_eval:\n",
        "        best_eval = eval_acc\n",
        "        save_model(state, step=epoch_idx)\n",
        "      logger.flush()\n",
        "  # Evaluate after training\n",
        "  test_acc = eval_model(test_loader, state, rng)\n",
        "  print(f'test_acc: {test_acc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vSMAOGysXtm"
      },
      "source": [
        "## Create train state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQtd9GoQRFtJ"
      },
      "outputs": [],
      "source": [
        "def create_train_state(\n",
        "    model, params, learning_rate\n",
        "):\n",
        "  optimizer = optax.adam(learning_rate)\n",
        "  return train_state.TrainState.create(\n",
        "      apply_fn=model.apply,\n",
        "      tx=optimizer,\n",
        "      params=params\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYnlWm6TsQ0-"
      },
      "source": [
        "## Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SW0BlojZ-a1c"
      },
      "outputs": [],
      "source": [
        "def save_model(state, step=0):\n",
        "  checkpoints.save_checkpoint(ckpt_dir='vit_jax_logs', target=state.params, step=step, overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WafQ6llqsAxq"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXDI4K55Rdan"
      },
      "outputs": [],
      "source": [
        "state = create_train_state(vit_model, vit_params, 3e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEW1gpM8_R66"
      },
      "outputs": [],
      "source": [
        "train_model(train_loader, val_loader, state, vit_rng, num_epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0_Fo6sKRJdt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
